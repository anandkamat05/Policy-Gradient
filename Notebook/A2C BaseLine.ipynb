{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "tf.set_random_seed(2)  # reproducible\n",
    "\n",
    "# Superparameters\n",
    "OUTPUT_GRAPH = True\n",
    "MAX_EPISODE = 300\n",
    "DISPLAY_REWARD_THRESHOLD = 200  # renders environment if total episode reward is greater then this threshold\n",
    "MAX_EP_STEPS = 1000   # maximum time step in one episode\n",
    "RENDER = False  # rendering wastes time\n",
    "GAMMA = 0.9     # reward discount in TD error\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.01     # learning rate for critic\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(1)  # reproducible\n",
    "env = env.unwrapped\n",
    "\n",
    "N_F = env.observation_space.shape[0]\n",
    "N_A = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    def __init__(self, sess, n_features, n_actions, lr=0.001):\n",
    "        self.sess = sess\n",
    "\n",
    "        self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "        self.a = tf.placeholder(tf.int32, None, \"act\")\n",
    "        self.td_error = tf.placeholder(tf.float32, None, \"td_error\")  # TD_error\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=20,    # number of hidden units\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),    # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            self.acts_prob = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=n_actions,    # output units\n",
    "                activation=tf.nn.softmax,   # get action probabilities\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='acts_prob'\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('exp_v'):\n",
    "            log_prob = tf.log(self.acts_prob[0, self.a])\n",
    "            self.exp_v = tf.reduce_mean(log_prob * self.td_error)  # advantage (TD_error) guided loss\n",
    "\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.exp_v)  # minimize(-exp_v) = maximize(exp_v)\n",
    "\n",
    "    def learn(self, s, a, td):\n",
    "        s = s[np.newaxis, :]\n",
    "        feed_dict = {self.s: s, self.a: a, self.td_error: td}\n",
    "        _, exp_v = self.sess.run([self.train_op, self.exp_v], feed_dict)\n",
    "        return exp_v\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]\n",
    "        probs = self.sess.run(self.acts_prob, {self.s: s})   # get probabilities for all actions\n",
    "        return np.random.choice(np.arange(probs.shape[1]), p=probs.ravel())   # return a int\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(object):\n",
    "    def __init__(self, sess, n_features, lr=0.01):\n",
    "        self.sess = sess\n",
    "\n",
    "        self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "        self.v_ = tf.placeholder(tf.float32, [1, 1], \"v_next\")\n",
    "        self.r = tf.placeholder(tf.float32, None, 'r')\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=20,  # number of hidden units\n",
    "                activation=tf.nn.relu,  # None\n",
    "                # have to be linear to make sure the convergence of actor.\n",
    "                # But linear approximator seems hardly learns the correct Q.\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            self.v = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=1,  # output units\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='V'\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('squared_TD_error'):\n",
    "            self.td_error = self.r + GAMMA * self.v_ - self.v\n",
    "            self.loss = tf.square(self.td_error)    # TD_error = (r+gamma*V_next) - V_eval\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "\n",
    "    def learn(self, s, r, s_):\n",
    "        s, s_ = s[np.newaxis, :], s_[np.newaxis, :]\n",
    "\n",
    "        v_ = self.sess.run(self.v, {self.s: s_})\n",
    "        td_error, _ = self.sess.run([self.td_error, self.train_op],\n",
    "                                          {self.s: s, self.v_: v_, self.r: r})\n",
    "        return td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "actor = Actor(sess, n_features=N_F, n_actions=N_A, lr=LR_A)\n",
    "critic = Critic(sess, n_features=N_F, lr=LR_C)     # we need a good teacher, so the teacher should learn faster than the actor\n",
    "\n",
    "writer = tf.summary.FileWriter('./graphs', tf.get_default_graph())\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "writer.close()\n",
    "# if OUTPUT_GRAPH:\n",
    "#     tf.summary.FileWriter(\"logs/\", sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   reward: -7\n",
      "episode: 1   reward: -6\n",
      "episode: 2   reward: -5\n",
      "episode: 3   reward: -5\n",
      "episode: 4   reward: -5\n",
      "episode: 5   reward: -5\n",
      "episode: 6   reward: -6\n",
      "episode: 7   reward: -6\n",
      "episode: 8   reward: -6\n",
      "episode: 9   reward: -6\n",
      "episode: 10   reward: -6\n",
      "episode: 11   reward: -6\n",
      "episode: 12   reward: -5\n",
      "episode: 13   reward: -5\n",
      "episode: 14   reward: -5\n",
      "episode: 15   reward: -5\n",
      "episode: 16   reward: -5\n",
      "episode: 17   reward: -5\n",
      "episode: 18   reward: -5\n",
      "episode: 19   reward: -4\n",
      "episode: 20   reward: -4\n",
      "episode: 21   reward: -3\n",
      "episode: 22   reward: -3\n",
      "episode: 23   reward: -3\n",
      "episode: 24   reward: -4\n",
      "episode: 25   reward: -3\n",
      "episode: 26   reward: -3\n",
      "episode: 27   reward: -4\n",
      "episode: 28   reward: -3\n",
      "episode: 29   reward: -3\n",
      "episode: 30   reward: -2\n",
      "episode: 31   reward: -2\n",
      "episode: 32   reward: -2\n",
      "episode: 33   reward: -1\n",
      "episode: 34   reward: -1\n",
      "episode: 35   reward: -2\n",
      "episode: 36   reward: -1\n",
      "episode: 37   reward: -2\n",
      "episode: 38   reward: 0\n",
      "episode: 39   reward: 0\n",
      "episode: 40   reward: 0\n",
      "episode: 41   reward: 0\n",
      "episode: 42   reward: 0\n",
      "episode: 43   reward: 0\n",
      "episode: 44   reward: 0\n",
      "episode: 45   reward: -1\n",
      "episode: 46   reward: -1\n",
      "episode: 47   reward: -1\n",
      "episode: 48   reward: -2\n",
      "episode: 49   reward: -2\n",
      "episode: 50   reward: -2\n",
      "episode: 51   reward: -2\n",
      "episode: 52   reward: -2\n",
      "episode: 53   reward: -1\n",
      "episode: 54   reward: 0\n",
      "episode: 55   reward: 1\n",
      "episode: 56   reward: 3\n",
      "episode: 57   reward: 5\n",
      "episode: 58   reward: 5\n",
      "episode: 59   reward: 7\n",
      "episode: 60   reward: 7\n",
      "episode: 61   reward: 7\n",
      "episode: 62   reward: 7\n",
      "episode: 63   reward: 6\n",
      "episode: 64   reward: 6\n",
      "episode: 65   reward: 7\n",
      "episode: 66   reward: 6\n",
      "episode: 67   reward: 5\n",
      "episode: 68   reward: 5\n",
      "episode: 69   reward: 6\n",
      "episode: 70   reward: 5\n",
      "episode: 71   reward: 5\n",
      "episode: 72   reward: 6\n",
      "episode: 73   reward: 6\n",
      "episode: 74   reward: 6\n",
      "episode: 75   reward: 5\n",
      "episode: 76   reward: 5\n",
      "episode: 77   reward: 5\n",
      "episode: 78   reward: 5\n",
      "episode: 79   reward: 7\n",
      "episode: 80   reward: 10\n",
      "episode: 81   reward: 9\n",
      "episode: 82   reward: 9\n",
      "episode: 83   reward: 9\n",
      "episode: 84   reward: 8\n",
      "episode: 85   reward: 8\n",
      "episode: 86   reward: 8\n",
      "episode: 87   reward: 8\n",
      "episode: 88   reward: 8\n",
      "episode: 89   reward: 8\n",
      "episode: 90   reward: 8\n",
      "episode: 91   reward: 7\n",
      "episode: 92   reward: 9\n",
      "episode: 93   reward: 9\n",
      "episode: 94   reward: 8\n",
      "episode: 95   reward: 8\n",
      "episode: 96   reward: 9\n",
      "episode: 97   reward: 10\n",
      "episode: 98   reward: 9\n",
      "episode: 99   reward: 9\n",
      "episode: 100   reward: 15\n",
      "episode: 101   reward: 15\n",
      "episode: 102   reward: 15\n",
      "episode: 103   reward: 15\n",
      "episode: 104   reward: 16\n",
      "episode: 105   reward: 17\n",
      "episode: 106   reward: 18\n",
      "episode: 107   reward: 17\n",
      "episode: 108   reward: 19\n",
      "episode: 109   reward: 19\n",
      "episode: 110   reward: 23\n",
      "episode: 111   reward: 23\n",
      "episode: 112   reward: 23\n",
      "episode: 113   reward: 24\n",
      "episode: 114   reward: 24\n",
      "episode: 115   reward: 24\n",
      "episode: 116   reward: 24\n",
      "episode: 117   reward: 25\n",
      "episode: 118   reward: 26\n",
      "episode: 119   reward: 27\n",
      "episode: 120   reward: 27\n",
      "episode: 121   reward: 28\n",
      "episode: 122   reward: 30\n",
      "episode: 123   reward: 30\n",
      "episode: 124   reward: 30\n",
      "episode: 125   reward: 29\n",
      "episode: 126   reward: 33\n",
      "episode: 127   reward: 34\n",
      "episode: 128   reward: 37\n",
      "episode: 129   reward: 43\n",
      "episode: 130   reward: 47\n",
      "episode: 131   reward: 51\n",
      "episode: 132   reward: 55\n",
      "episode: 133   reward: 65\n",
      "episode: 134   reward: 93\n",
      "episode: 135   reward: 98\n",
      "episode: 136   reward: 104\n",
      "episode: 137   reward: 104\n",
      "episode: 138   reward: 99\n",
      "episode: 139   reward: 98\n",
      "episode: 140   reward: 95\n",
      "episode: 141   reward: 94\n",
      "episode: 142   reward: 91\n",
      "episode: 143   reward: 92\n",
      "episode: 144   reward: 92\n",
      "episode: 145   reward: 94\n",
      "episode: 146   reward: 97\n",
      "episode: 147   reward: 99\n",
      "episode: 148   reward: 97\n",
      "episode: 149   reward: 95\n",
      "episode: 150   reward: 96\n",
      "episode: 151   reward: 94\n",
      "episode: 152   reward: 95\n",
      "episode: 153   reward: 96\n",
      "episode: 154   reward: 100\n",
      "episode: 155   reward: 105\n",
      "episode: 156   reward: 112\n",
      "episode: 157   reward: 119\n",
      "episode: 158   reward: 130\n",
      "episode: 159   reward: 151\n",
      "episode: 160   reward: 157\n",
      "episode: 161   reward: 163\n",
      "episode: 162   reward: 163\n",
      "episode: 163   reward: 167\n",
      "episode: 164   reward: 172\n",
      "episode: 165   reward: 169\n",
      "episode: 166   reward: 168\n",
      "episode: 167   reward: 164\n",
      "episode: 168   reward: 161\n",
      "episode: 169   reward: 158\n",
      "episode: 170   reward: 154\n",
      "episode: 171   reward: 151\n",
      "episode: 172   reward: 150\n",
      "episode: 173   reward: 149\n",
      "episode: 174   reward: 150\n",
      "episode: 175   reward: 147\n",
      "episode: 176   reward: 146\n",
      "episode: 177   reward: 139\n",
      "episode: 178   reward: 136\n",
      "episode: 179   reward: 134\n",
      "episode: 180   reward: 134\n",
      "episode: 181   reward: 137\n",
      "episode: 182   reward: 135\n",
      "episode: 183   reward: 134\n",
      "episode: 184   reward: 131\n",
      "episode: 185   reward: 126\n",
      "episode: 186   reward: 121\n",
      "episode: 187   reward: 117\n",
      "episode: 188   reward: 116\n",
      "episode: 189   reward: 111\n",
      "episode: 190   reward: 111\n",
      "episode: 191   reward: 113\n",
      "episode: 192   reward: 113\n",
      "episode: 193   reward: 113\n",
      "episode: 194   reward: 113\n",
      "episode: 195   reward: 110\n",
      "episode: 196   reward: 106\n",
      "episode: 197   reward: 102\n",
      "episode: 198   reward: 100\n",
      "episode: 199   reward: 97\n",
      "episode: 200   reward: 94\n",
      "episode: 201   reward: 92\n",
      "episode: 202   reward: 90\n",
      "episode: 203   reward: 88\n",
      "episode: 204   reward: 86\n",
      "episode: 205   reward: 87\n",
      "episode: 206   reward: 88\n",
      "episode: 207   reward: 88\n",
      "episode: 208   reward: 89\n",
      "episode: 209   reward: 88\n",
      "episode: 210   reward: 88\n",
      "episode: 211   reward: 90\n",
      "episode: 212   reward: 91\n",
      "episode: 213   reward: 93\n",
      "episode: 214   reward: 97\n",
      "episode: 215   reward: 102\n",
      "episode: 216   reward: 106\n",
      "episode: 217   reward: 119\n",
      "episode: 218   reward: 119\n",
      "episode: 219   reward: 124\n",
      "episode: 220   reward: 129\n",
      "episode: 221   reward: 127\n",
      "episode: 222   reward: 128\n",
      "episode: 223   reward: 126\n",
      "episode: 224   reward: 124\n",
      "episode: 225   reward: 123\n",
      "episode: 226   reward: 122\n",
      "episode: 227   reward: 126\n",
      "episode: 228   reward: 134\n",
      "episode: 229   reward: 136\n",
      "episode: 230   reward: 136\n",
      "episode: 231   reward: 135\n",
      "episode: 232   reward: 134\n",
      "episode: 233   reward: 131\n",
      "episode: 234   reward: 129\n",
      "episode: 235   reward: 127\n",
      "episode: 236   reward: 125\n",
      "episode: 237   reward: 123\n",
      "episode: 238   reward: 121\n",
      "episode: 239   reward: 119\n",
      "episode: 240   reward: 116\n",
      "episode: 241   reward: 115\n",
      "episode: 242   reward: 117\n",
      "episode: 243   reward: 118\n",
      "episode: 244   reward: 121\n",
      "episode: 245   reward: 122\n",
      "episode: 246   reward: 127\n",
      "episode: 247   reward: 132\n",
      "episode: 248   reward: 137\n",
      "episode: 249   reward: 138\n",
      "episode: 250   reward: 140\n",
      "episode: 251   reward: 148\n",
      "episode: 252   reward: 152\n",
      "episode: 253   reward: 151\n",
      "episode: 254   reward: 154\n",
      "episode: 255   reward: 153\n",
      "episode: 256   reward: 150\n",
      "episode: 257   reward: 148\n",
      "episode: 258   reward: 146\n",
      "episode: 259   reward: 146\n",
      "episode: 260   reward: 157\n",
      "episode: 261   reward: 166\n",
      "episode: 262   reward: 164\n",
      "episode: 263   reward: 163\n",
      "episode: 264   reward: 160\n",
      "episode: 265   reward: 156\n",
      "episode: 266   reward: 153\n",
      "episode: 267   reward: 150\n",
      "episode: 268   reward: 150\n",
      "episode: 269   reward: 150\n",
      "episode: 270   reward: 155\n",
      "episode: 271   reward: 159\n",
      "episode: 272   reward: 162\n",
      "episode: 273   reward: 163\n",
      "episode: 274   reward: 164\n",
      "episode: 275   reward: 161\n",
      "episode: 276   reward: 158\n",
      "episode: 277   reward: 154\n",
      "episode: 278   reward: 150\n",
      "episode: 279   reward: 143\n",
      "episode: 280   reward: 138\n",
      "episode: 281   reward: 131\n",
      "episode: 282   reward: 126\n",
      "episode: 283   reward: 120\n",
      "episode: 284   reward: 115\n",
      "episode: 285   reward: 111\n",
      "episode: 286   reward: 107\n",
      "episode: 287   reward: 105\n",
      "episode: 288   reward: 104\n",
      "episode: 289   reward: 104\n",
      "episode: 290   reward: 109\n",
      "episode: 291   reward: 121\n",
      "episode: 292   reward: 123\n",
      "episode: 293   reward: 123\n",
      "episode: 294   reward: 123\n",
      "episode: 295   reward: 123\n",
      "episode: 296   reward: 124\n",
      "episode: 297   reward: 123\n",
      "episode: 298   reward: 123\n",
      "episode: 299   reward: 124\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(MAX_EPISODE):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    track_r = []\n",
    "    while True:\n",
    "        if RENDER: env.render()\n",
    "\n",
    "        a = actor.choose_action(s)\n",
    "\n",
    "        s_, r, done, info = env.step(a)\n",
    "\n",
    "        if done: r = -20\n",
    "\n",
    "        track_r.append(r)\n",
    "\n",
    "        td_error = critic.learn(s, r, s_)  # gradient = grad[r + gamma * V(s_) - V(s)]\n",
    "        actor.learn(s, a, td_error)     # true_gradient = grad[logPi(s,a) * td_error]\n",
    "\n",
    "        s = s_\n",
    "        t += 1\n",
    "\n",
    "        if done or t >= MAX_EP_STEPS:\n",
    "            ep_rs_sum = sum(track_r)\n",
    "\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.95 + ep_rs_sum * 0.05\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            print(\"\\repisode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
